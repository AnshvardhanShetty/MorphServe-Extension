{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MorphServe Extension — Interactive Notebook\n",
    "\n",
    "Demonstrates that **contiguous middle-block swapping** matches or beats scattered LIS-based selection for runtime FP16↔INT4 layer morphing.\n",
    "\n",
    "This notebook imports from the `morphserve/` package. Make sure you've installed dependencies:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '..')\n\nimport torch\nimport numpy as np\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nprint(f\"GPU available: {torch.cuda.is_available()}\")\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Calibration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morphserve.models import load_fp16_model, load_int4_model, load_calibration_data\n",
    "\n",
    "model_fp16, tokenizer, num_layers = load_fp16_model()\n",
    "model_int4 = load_int4_model()\n",
    "inputs_list = load_calibration_data(tokenizer, n_texts=20, max_length=512)\n",
    "inputs = inputs_list[0]\n",
    "print(f\"Sequence length: {inputs['input_ids'].shape[1]} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Layer Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morphserve.sensitivity import (\n",
    "    compute_lts_scores, compute_lrs_scores,\n",
    "    compute_mds_scores, compute_lis_scores\n",
    ")\n",
    "\n",
    "print(\"Computing LTS...\")\n",
    "lts_scores, layer_outputs = compute_lts_scores(model_fp16, inputs)\n",
    "for i, s in enumerate(lts_scores):\n",
    "    print(f\"  Layer {i:2d}: LTS = {s:.6f}\")\n",
    "print(\"Higher LTS = layer transforms less = safer to swap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing LRS...\")\n",
    "lrs_scores = compute_lrs_scores(model_fp16, model_int4, inputs, layer_outputs)\n",
    "for i, s in enumerate(lrs_scores):\n",
    "    print(f\"  Layer {i:2d}: LRS = {s:.6f}\")\n",
    "print(\"Higher LRS = quantization changes layer less = safer to swap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing MDS (this takes a minute)...\")\n",
    "mds_scores = compute_mds_scores(model_fp16, model_int4, inputs)\n",
    "for i, s in enumerate(mds_scores):\n",
    "    print(f\"  Layer {i:2d}: MDS = {s:.6f}\")\n",
    "print(\"Higher MDS = swapping barely affects output = safer to swap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_scores = compute_lis_scores(lts_scores, lrs_scores, mds_scores)\n",
    "\n",
    "lis_ranking = sorted(range(num_layers), key=lambda i: lis_scores[i], reverse=True)\n",
    "print(\"LIS swapping order (safest first):\")\n",
    "print(lis_ranking)\n",
    "\n",
    "print(f\"\\n{'Rank':<6}{'Layer':<8}{'LIS':<10}{'LTS':<10}{'LRS':<10}{'MDS':<10}\")\n",
    "print(\"-\" * 54)\n",
    "for rank, idx in enumerate(lis_ranking):\n",
    "    print(f\"{rank:<6}{idx:<8}{lis_scores[idx]:<10.6f}\"\n",
    "          f\"{lts_scores[idx]:<10.6f}{lrs_scores[idx]:<10.6f}{mds_scores[idx]:<10.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morphserve.visualization import plot_sensitivity\n",
    "\n",
    "plot_sensitivity(lts_scores, lrs_scores, mds_scores, lis_scores,\n",
    "                 save_path='../figures/layer_sensitivity.png',\n",
    "                 title_suffix=' — TinyLlama 1.1B')\n",
    "\n",
    "from IPython.display import Image\n",
    "Image('../figures/layer_sensitivity.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Swapping Strategy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morphserve.strategies import (\n",
    "    compute_perplexity, test_ordering,\n",
    "    greedy_lis_order, find_best_contiguous_block\n",
    ")\n",
    "\n",
    "ppl_fp16 = compute_perplexity(model_fp16, inputs['input_ids'])\n",
    "print(f\"FP16 baseline perplexity: {ppl_fp16:.4f}\")\n",
    "\n",
    "front_to_back = list(range(num_layers))\n",
    "back_to_front = list(range(num_layers - 1, -1, -1))\n",
    "swap_counts = [1, 2, 4, 8, 11, 16, 22]\n",
    "\n",
    "print(f\"\\n{'N swapped':<12}{'LIS':<12}{'Front-Back':<12}{'Back-Front':<12}\")\n",
    "print(\"-\" * 48)\n",
    "for n in swap_counts:\n",
    "    ppl_lis = test_ordering(lis_ranking, n, model_fp16, model_int4, inputs['input_ids'])\n",
    "    ppl_ftb = test_ordering(front_to_back, n, model_fp16, model_int4, inputs['input_ids'])\n",
    "    ppl_btf = test_ordering(back_to_front, n, model_fp16, model_int4, inputs['input_ids'])\n",
    "    print(f\"{n:<12}{ppl_lis:<12.4f}{ppl_ftb:<12.4f}{ppl_btf:<12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing greedy LIS ordering (this takes a while)...\\n\")\n",
    "greedy_order, greedy_ppls = greedy_lis_order(\n",
    "    model_fp16, model_int4, inputs, lts_scores, lrs_scores\n",
    ")\n",
    "print(f\"\\nGreedy order: {greedy_order}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full perplexity curves\n",
    "swap_counts_full = list(range(1, num_layers + 1))\n",
    "ftb_ppls = [test_ordering(front_to_back, n, model_fp16, model_int4, inputs['input_ids'])\n",
    "            for n in swap_counts_full]\n",
    "static_ppls = [test_ordering(lis_ranking, n, model_fp16, model_int4, inputs['input_ids'])\n",
    "               for n in swap_counts_full]\n",
    "\n",
    "from morphserve.visualization import plot_strategy_comparison\n",
    "plot_strategy_comparison(greedy_ppls, static_ppls, ftb_ppls, ppl_fp16,\n",
    "                         greedy_order=greedy_order,\n",
    "                         save_path='../figures/strategy_comparison.png')\n",
    "\n",
    "Image('../figures/strategy_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Contiguous Block Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_swap_counts = [1, 2, 4, 8, 11, 16]\n",
    "\n",
    "print(f\"{'N swapped':<12}{'Best block':<20}{'Block PPL':<14}{'LIS PPL':<14}{'FtB PPL':<14}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "for n in block_swap_counts:\n",
    "    best_start, best_ppl = find_best_contiguous_block(\n",
    "        model_fp16, model_int4, n, inputs['input_ids']\n",
    "    )\n",
    "    ppl_lis = test_ordering(greedy_order, n, model_fp16, model_int4, inputs['input_ids'])\n",
    "    ppl_ftb = test_ordering(front_to_back, n, model_fp16, model_int4, inputs['input_ids'])\n",
    "    print(f\"{n:<12}[{best_start}-{best_start+n-1}]{'':<14}{best_ppl:<14.4f}\"\n",
    "          f\"{ppl_lis:<14.4f}{ppl_ftb:<14.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch.nn.functional as F\nimport numpy as np\nfrom morphserve.strategies import swap_layers\n\n# Middle vs Edge comparison\ninputs2 = inputs_list[1] if len(inputs_list) > 1 else inputs\n\nprint(\"Same number of layers swapped: MIDDLE vs EDGES\\n\")\nprint(f\"{'N swapped':<12}{'Middle block':<18}{'Edge layers':<18}{'Difference':<12}\")\nprint(\"-\" * 60)\n\nfor n in [2, 4, 6, 8]:\n    half = n // 2\n    mid_start = num_layers // 2 - (n // 2)\n    mid_block = list(range(mid_start, mid_start + n))\n    edge_layers = list(range(half)) + list(range(num_layers - half, num_layers))\n\n    test_inputs = [inputs, inputs2]\n    mid_ppls, edge_ppls = [], []\n    for inp in test_inputs:\n        with swap_layers(model_fp16, model_int4, mid_block):\n            mid_ppls.append(compute_perplexity(model_fp16, inp['input_ids']))\n        with swap_layers(model_fp16, model_int4, edge_layers):\n            edge_ppls.append(compute_perplexity(model_fp16, inp['input_ids']))\n\n    avg_mid = np.mean(mid_ppls)\n    avg_edge = np.mean(edge_ppls)\n    print(f\"{n:<12}{avg_mid:<18.4f}{avg_edge:<18.4f}{avg_edge - avg_mid:+.4f}\")\n    print(f\"{'':12}  middle: {mid_block}\")\n    print(f\"{'':12}  edges:  {edge_layers}\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CUDA Benchmark (Overlap Proof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morphserve.benchmark import stats, benchmark_overlap, benchmark_scattered_vs_block\n",
    "\n",
    "ref_weight = model_fp16.model.layers[10].self_attn.q_proj.weight.data\n",
    "\n",
    "def simulate_inference(n_steps=50):\n",
    "    x = torch.randn(1, 2048, dtype=torch.float16, device='cuda')\n",
    "    for _ in range(n_steps):\n",
    "        x = x @ ref_weight.T\n",
    "        x = x / x.norm()\n",
    "    return x\n",
    "\n",
    "# Warmup\n",
    "for _ in range(10):\n",
    "    simulate_inference()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Prepare swap buffer\n",
    "layer = model_fp16.model.layers[10]\n",
    "flat = torch.cat([p.data.flatten() for p in layer.parameters()])\n",
    "host_pinned = torch.empty_like(flat, device='cpu', pin_memory=True)\n",
    "host_pinned.copy_(flat.cpu())\n",
    "gpu_buf = flat.clone().cuda()\n",
    "\n",
    "no_swap = benchmark_overlap(simulate_inference, None, n_iter=20)\n",
    "with_swap = benchmark_overlap(\n",
    "    simulate_inference,\n",
    "    lambda: gpu_buf.copy_(host_pinned, non_blocking=True),\n",
    "    n_iter=20\n",
    ")\n",
    "\n",
    "print(f\"Inference (no swap):          {stats(no_swap['compute_times'])}\")\n",
    "print(f\"Inference (overlapped swap):  {stats(with_swap['compute_times'])}\")\n",
    "print(f\"Swap time (separate stream):  {stats(with_swap['swap_times'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = benchmark_scattered_vs_block(\n",
    "    model_fp16,\n",
    "    scattered_indices=[5, 10, 11, 17],\n",
    "    block_indices=[8, 9, 10, 11],\n",
    "    n_iter=50\n",
    ")\n",
    "\n",
    "print(f\"Scattered: {stats(result['scattered_times'])}\")\n",
    "print(f\"Block:     {stats(result['block_times'])}\")\n",
    "print(f\"Speedup:   {result['speedup']:.2f}x\")\n",
    "print(f\"Jitter reduction: {result['jitter_reduction']:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Burst Serving Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morphserve.simulation import MinimalServingSimulator\n",
    "from morphserve.visualization import plot_burst_results\n",
    "\n",
    "# Warmup\n",
    "for _ in range(5):\n",
    "    with torch.no_grad():\n",
    "        model_fp16(tokenizer(\"Hello\", return_tensors=\"pt\").input_ids.to(\"cuda\"))\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "results = {}\n",
    "for policy in ['none', 'scattered', 'block']:\n",
    "    sim = MinimalServingSimulator(model_fp16, model_int4, tokenizer, max_kv_blocks=50)\n",
    "    ttfts, tpots, phases, schedule = sim.run_burst_experiment(\n",
    "        swap_policy=policy, decode_steps=10\n",
    "    )\n",
    "    results[policy] = {'ttfts': ttfts, 'tpots': tpots, 'phases': phases, 'schedule': schedule}\n",
    "    print(f\"Policy: {policy}\")\n",
    "    print(f\"  TTFT p50: {np.percentile(ttfts, 50):.2f}ms  p95: {np.percentile(ttfts, 95):.2f}ms\")\n",
    "    print(f\"  TPOT p50: {np.percentile(tpots, 50):.2f}ms  p95: {np.percentile(tpots, 95):.2f}ms\")\n",
    "\n",
    "plot_burst_results(results, save_path='../figures/burst_serving_results.png')\n",
    "Image('../figures/burst_serving_results.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}